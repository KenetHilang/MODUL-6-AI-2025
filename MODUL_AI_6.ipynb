{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCX7Cz5AKqwr"
      },
      "source": [
        "# **Modul 6 AI LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGhF4kSiK8oz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz0e3Am2KxZB"
      },
      "source": [
        "**Nama**: Michael Kenneth Salim <br>\n",
        "**NRP**: 5027231008"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpaW5VR6LAU-"
      },
      "source": [
        "## **Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DzWdSyxRB5ZG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction, DataCollatorWithPadding\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPg9ZlEiKkOy"
      },
      "source": [
        "## **EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y40yY9dXKj93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "4fe6c651-3cba-4265-fca7-77bc5bc92bd4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4274157143>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN8YMBI2Q8B2"
      },
      "outputs": [],
      "source": [
        "df_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pnN9atkRBZY"
      },
      "outputs": [],
      "source": [
        "df_test.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u13vViKtRG3t"
      },
      "outputs": [],
      "source": [
        "df_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTTlizxARLit"
      },
      "outputs": [],
      "source": [
        "df_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5s5gG2ZWwVo"
      },
      "source": [
        "### **Visualisasi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvdXDxorR3by"
      },
      "outputs": [],
      "source": [
        "emotion_columns = ['amusement', 'anger', 'annoyance', 'caring', 'confusion',\n",
        "                   'disappointment', 'disgust', 'embarrassment', 'excitement',\n",
        "                   'fear', 'gratitude', 'joy', 'love', 'sadness']\n",
        "\n",
        "emotion_counts = df_train[emotion_columns].sum()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "colors = ['#FF6B6B', '#FF8E53', '#FF6B9D', '#4ECDC4', '#45B7D1',\n",
        "          '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F',\n",
        "          '#85C1E9', '#F8C471', '#EC7063', '#AED6F1']\n",
        "\n",
        "bars = plt.bar(range(len(emotion_counts)), emotion_counts.values,\n",
        "               color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "plt.xlabel('Emotions', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
        "plt.title('Distribution of Emotions in Training Data', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "plt.xticks(range(len(emotion_counts)), emotion_counts.index, rotation=45, ha='right')\n",
        "\n",
        "for i, (emotion, count) in enumerate(zip(emotion_counts.index, emotion_counts.values)):\n",
        "    plt.text(i, count + max(emotion_counts.values) * 0.01, str(count),\n",
        "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7BME17JR2FU"
      },
      "source": [
        "## **Data Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G6c8PIrVF7o"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IY3dmwvVXc3"
      },
      "outputs": [],
      "source": [
        "df_train['cleaned_text'] = df_train['text'].apply(clean_text)\n",
        "df_test['cleaned_text'] = df_test['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUdp2Q_BVn6p"
      },
      "outputs": [],
      "source": [
        "df_train[['text', 'cleaned_text']][:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWhB0H0VGg06"
      },
      "source": [
        "### Split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, texts, labels=None, tokenizer=None, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "6-AsP1wYg6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdULvrLGF30x"
      },
      "outputs": [],
      "source": [
        "X = df_train['cleaned_text'].values\n",
        "y = df_train[emotion_columns].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "D1fKhOaIh3Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train size: {len(X_train)}\")\n",
        "print(f\"Validation size: {len(X_val)}\")\n",
        "print(f\"Test size: {len(df_test)}\")"
      ],
      "metadata": {
        "id": "ios2xUq7i9eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCyyMVNUGjHz"
      },
      "source": [
        "### **Initiate 1st Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOaNoctKWACW"
      },
      "outputs": [],
      "source": [
        "model_name = \"google-bert/bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 256\n",
        "\n",
        "train_data = EmotionDataset(X_train, y_train, tokenizer, max_length)\n",
        "val_data = EmotionDataset(X_val, y_val, tokenizer, max_length)\n",
        "\n",
        "test_data = EmotionDataset(df_test['cleaned_text'].values, None, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "gI9QTABRjBs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idtolabel = {i: label for i, label in enumerate(emotion_columns)}\n",
        "labeltoid = {label: i for i, label in enumerate(emotion_columns)}\n",
        "num_labels = len(emotion_columns)"
      ],
      "metadata": {
        "id": "gO9T4l9ZhEUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYWfpFLXEBWA"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    id2label=idtolabel,\n",
        "    label2id=labeltoid,\n",
        ")\n",
        "\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")\n",
        "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOuMPrA8Hvkw"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='emotion_classifier',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=True,\n",
        "    report_to=[],\n",
        "    hub_model_id=\"KenetHilang/emotion-classifier\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vggkGX0nIZJY"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    predictions, labels = eval_pred\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    y_pred = (probs > 0.5).int().numpy()\n",
        "    y_true = labels\n",
        "\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return {\n",
        "        'f1': f1_micro,\n",
        "        'f1_macro': f1_macro,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od4rSHYIIaXm"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C5kZJFHjyqs"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9fVpoPh9q6R"
      },
      "source": [
        "## **Upload Model to HuggingFace**\n",
        "\n",
        "After training, the model will be automatically uploaded to HuggingFace due to the `push_to_hub=True` setting in TrainingArguments.\n",
        "\n",
        "If you need to manually upload the model later, you can run:\n",
        "```python\n",
        "# trainer.push_to_hub()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpseAsKf9q6V"
      },
      "source": [
        "## **Generate Test Predictions for Kaggle**\n",
        "\n",
        "Now let's generate predictions for the test data and create the submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkQ1kAZD9q6X"
      },
      "outputs": [],
      "source": [
        "# Prepare test data\n",
        "test_texts = df_test['cleaned_text'].tolist()\n",
        "test_data = Dataset.from_dict({'text': test_texts})\n",
        "test_data = test_data.map(tokenize_function, batched=True)\n",
        "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "# Generate predictions\n",
        "print(\"Generating predictions for test data...\")\n",
        "predictions = trainer.predict(test_data)\n",
        "test_predictions = torch.nn.Sigmoid()(torch.tensor(predictions.predictions))\n",
        "test_predictions = (test_predictions > 0.5).int().numpy()\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame(test_predictions, columns=emotion_columns)\n",
        "submission.insert(0, 'id', df_test['id'])  # Use the actual ID column from test.csv\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: submission.csv\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(submission.head())\n",
        "\n",
        "# Check submission format\n",
        "print(\"\\nSubmission statistics:\")\n",
        "print(submission[emotion_columns].describe())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}